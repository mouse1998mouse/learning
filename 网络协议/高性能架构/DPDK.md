## DPDK
这是一套软件架构的的高性能网络IO开发框架

##### 背景
网速一直在飞升，网络技术的发展也在2G-5G中进行演变。这就要求着单机网络的IO必须要可以跟上时代的发展。网卡的快速发展，cpu从单核到多核的发展让服务器的单机能力到达一个新的顶点。云的出现，让网络功能虚拟化，共享硬件成为趋势，所以一套基于常用系统的核标准服务器的高性能网络IO开发框架应运而生。

##### IO瓶颈
根据经验，在C1（8核）上跑应用每1W包处理需要消耗1%软中断CPU，这意味着单机的上限是100万PPS（Packet Per Second）。假设，我们要跑满10GE网卡，每个包64字节，这就需要2000万PPS（注：以太网万兆网卡速度上限是1488万PPS，因为最小帧大小为84B，100G是2亿PPS，即每个包的处理耗时不能超过50纳秒。而一次cache miss（缓存未命中），不管是TLB还是数据\指令cache，会内存读取的时间是65纳秒。这是在没有复杂的业务逻辑上的条件下，收发包都已经如此艰难，所以我们要**控制Cache的命中率。**
主要问题如下：
- 传统的收发报文方式都必须采用硬中断来做通讯，每次硬中断大约消耗100微秒，这还不算因为终止上下文所带来的Cache Miss。
- 数据必须从内核态用户态之间切换拷贝带来大量CPU消耗，全局锁竞争。
- 收发包都有系统调用的开销。
- 内核工作在多核上，为可全局一致，即使采用Lock Free，也避免不了锁总线、内存屏障带来的性能损耗。
- 从网卡到业务进程，经过的路径太长，有些其实未必要的，例如netfilter框架，这些都带来一定的消耗，而且容易Cache Miss。

### DPDK的基本原理
从上面列出的问题局限可以看出，IO的实现方式、内核的瓶颈以及数据流过内核存在不可控因素，这些都是在内核中实现，内核才是导致瓶颈的原因所在。那么有没有一种方式，我们可以绕过内核，直接在用户态进行收发包的（处理）？所以现在主流的方法就是使用旁路网卡IO，直接绕过内核在用户态进行收发包来解决内核的瓶颈。
DPDK的原理示意图如下：

![1647241989(1)](0072368C782B404CA45DD251BFC9E57B)

以前的数据传输方式（左边）一般为 网卡-驱动-协议栈-Socket接口-业务
DPDK的数据方式为：基于UIO（Userspace IO）旁路数据。数据从网卡-DPDK轮训-DPDK基础库-业务

把一切限制在用户态的好处就是易于开发跟维护。


### DPDK的UIO
![qxvnj4bv0h](172E122908F14E82BECB3B30929398B5)
让驱动运行在用户态，Linux提供UIO机制。UIO可以通过read感知中断，通过mmap实现和网卡的通讯。

1. 开发运行在内核的UIO模块，因为硬中断只能在内核处理
2. 通过/dev/uioX读取中断
3. 通过mmap和外设共享内存

#### DPDK的核心：PMD
所谓PMD：指的是DPDK的UIO驱动屏蔽了硬件发出中断，然后在用户态采用主动轮询的方式。
UIO旁路了内核，主动轮询去掉硬中断，DPDK从而可以在用户态做收发包处理。带来Zero Copy、无系统调用的好处，同步处理减少上下文切换带来的Cache Miss。但是如果网络空闲时，cpu会出现长期空转的情况，带来一系列的能耗问题，这个时候就有了一种叫Interrupt DPDK模式。

#### Interrupt DPDK模式
![pg3d428gpr](B3E032C7ABD4401C877A7DC4BC6E7436)
没有包的时候回进入睡眠，改为中断通知。并且可以和其他进程共享同个CPU Core，但是DPDK会有更高的调度优先级。

### 高性能的DPDK
##### 使用了HugePage减少TLB Miss
DPDK采用HugePage，在x86-64下支持2MB、1GB的页大小，几何级的降低了页表项的大小，从而减少TLB-Miss。并提供了内存池（Mempool）、MBuf、无锁环（Ring）、Bitmap等基础库。根据我们的实践，在数据平面（Data Plane）频繁的内存分配释放，必须使用内存池，不能直接使用rte_malloc，DPDK的内存分配实现非常简陋，不如ptmalloc。

##### SNA
软件结构去中心化，尽量避免全局共享，带来全局竞争，失去横向扩展的能力。NUMA体系下不跨Node远程使用内存。

##### 不使用慢速API等等






