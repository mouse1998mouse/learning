### 前提
引入特征函数非常的正常的事情，主要原因有：
- 在实际应用中，逐个测量事件空间中的各事件发生的概率（或者分布函数）是极端困难的，相反，对大多数分布而言，**矩（平均值、方差以及各种高阶矩等）** 往往是容易被测量的；
- 在问题变得复杂之后，再来计算矩（例如均值、方差等等）的时候，如果我们知道分布函数，那么我们要做的是求和与积分，而如果我们知道特征函数，在计算矩的时候，我们要做的只是微分，而通常，求导会比直接积分更容易，而且可以针对各阶矩有更统一的形式。

考虑到这两个原因，当我们进行傅里叶变换时，就很自然的想要引入特征函数了。

特征代表了某个事物，当所有特征都一样的时候，我们就可以理解成，这是同一个事物。

下面引入一个偏态的概念：
![偏态](https://www.zhihu.com/equation?tex=Skewness%3D%7B%5Cfrac+%7B%5Coverbrace%7BE%5BX%5E%7B3%7D%5D%7D%5E%7B%5Ctext%7B%E4%B8%89%E9%98%B6%E7%9F%A9%7D%7D-3%5Cmu+%5Csigma+%5E%7B2%7D-%5Cmu+%5E%7B3%7D%7D%7B%5Csigma+%5E%7B3%7D%7D%7D%5C%5C)
亦可以用另外一句话说：
各阶矩相同——>特征相同——>分布相同。

随机变量X的特征可以描述成：
![x的特征函数](https://www.zhihu.com/equation?tex=%5Cvarphi+_+X%28t%29+%3D+E%5Be%5E%7BitX%7D%5D%5C%5C)
其主要的推到过程为：
![推导过程](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cvarphi_X%28t%29+%26%3D+E%5Be%5E%7BitX%7D%5D%5C%5C+%26%3D+E%281+%2B+%5Cfrac%7Bit+X%7D%7B1%7D+-+%5Cfrac%7Bt%5E2+X%5E2%7D%7B2%21%7D+%2B+%5Ccdots+%2B+%5Cfrac%7B%28it%29%5En+X%5En%7D%7Bn%21%7D%29%5C%5C+%26%3D+E%281%29+%2B+E%28%5Cfrac%7Bit+X%7D%7B1%7D%29+-+E%28%5Cfrac%7Bt%5E2+X%5E2%7D%7B2%21%7D%29+%2B+%5Ccdots+%2B+E%28%5Cfrac%7B%28it%29%5En+X%5En%7D%7Bn%21%7D%29%5C%5C+%26%3D+1+%2B+%5Cfrac%7Bit+%5Coverbrace%7BE%5BX%5D%7D%5E%7B%5Ctext%7B%E4%B8%80%E9%98%B6%E7%9F%A9%7D%7D%7D%7B1%7D+-+%5Cfrac%7Bt%5E2+%5Coverbrace%7BE%5BX%5E2%5D%7D%5E%7B%5Ctext%7B%E4%BA%8C%E9%98%B6%E7%9F%A9%7D%7D%7D%7B2%21%7D+%2B+%5Ccdots+%2B+%5Cfrac%7B%28it%29%5En+%5Coverbrace%7BE%5BX%5En%5D%7D%5E%7B%5Ctext%7Bn%E9%98%B6%E7%9F%A9%7D%7D%7D%7Bn%21%7D%29+%5Cend%7Baligned%7D%5C%5C)

其中第二步，是进行了泰勒展开。
特征函数有个非常关键的性质。
- ==特征函数是共轭傅立叶变换==


另有一些个人的思考:（小结）
#### 特征值与特征向量
矩阵A对x做线性变换，变换的结果实际上只是对这个向量进行了伸缩变换。
&rarr;由此可以求矩阵能使哪些向量只发生拉伸，而且其拉伸程度很小。（特征值很小）
&rarr;特征值只是反应了特征向量的伸缩程度。==特征向量指明的方向才是最重要的。==